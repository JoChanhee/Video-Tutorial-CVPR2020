{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Distributed training of deep video models\n================================================\n\nTraining deep neural networks on videos is very time consuming. For example, training a state-of-the-art SlowFast network [Feichtenhofer18]_\non Kinetics400 dataset (with 240K 10-seconds short videos) using a server with 8 V100 GPUs takes more than 10 days. Slow training causes long research cycles\nand is not friendly for new comers and students to work on video related problems.\n\nUsing distributed training is a natural choice. Spreading the huge computation over multiple machines can speed up training\na lot. However, only a few open sourced Github repositories on video understanding support distributed training,\nand they often lack documentation for this feature.\nBesides, there is not much information/tutorial online on how to perform distributed training for deep video models.\n\nHence, we provide a simple tutorial here to demonstrate how to use our code to perform distributed training of SlowFast models\non Kinetics400 dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Distributed training\n--------------------\n\nThere are two ways in which we can distribute the workload of training a neural network across multiple devices,\ndata parallelism and model parallelism. Data parallelism refers to the case where each device stores a complete copy of the model.\nEach device works with a different part of the dataset, and the devices collectively update a shared model.\nWhen models are so large that they don't fit into device memory, then model parallelism is useful.\nHere, different devices are assigned the task of learning different parts of the model.\nIn this tutorial, we describe how to train a model with devices distributed across machines in a data parallel way.\n\nThere are some key concepts in distributed training, such as server, worker, scheduler, kvstore, etc.\nServer is a node to store model's parameters or gradients, and communicate with workers.\nWorker is a node actually performing training on a batch of training samples.\nBefore processing each batch, the workers pull weights from servers.\nThe workers also send gradients to the servers after each batch.\nScheduler is to set up the cluster for each node to communicate, and there is only one scheduler in the entire cluster\nKvstore, which is key-value store, is a critical component used for multi-device training.\nIt stores model parameters and optimizers, to receive gradient and update model.\nIn order to keep this tutorial concise, I wouldn't go into details.\nReaders can refer to MXNet `official documentation <https://mxnet.apache.org/api/faq/distributed_training>`_ for more information.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How to use our code to train a SlowFast model in a distributed manner?\n----------------------------------------------------------------------\n\nIn order to perform distributed training, you need to (1) prepare the cluster; (2) install MXNet; and (3) prepare your code\nand data.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need a cluster that each node can communicate with each other.\nThe first step is to generate ssh keys for each machine.\nFor better illustration, let's assume we have four machines, node1, node2, node3 and node4.\nWe use each of the machine as both server and worker.\nNote that, we can also have dedicated CPU machines as servers.\n\nFirst, ssh into node1 and type\n::\n\n    ssh-keygen -t rsa\n\nJust follow the default, you will have a file named ``id_rsa`` and a file named ``id_rsa.pub``, both under the ``~/.ssh/`` folder.\n``id_rsa`` is the private RSA key, and ``id_rsa.pub`` is its public key.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Second, copy both files (``id_rsa`` and ``id_rsa.pub``) of node1 to all other machines.\nFor each machine, you will find an ``authorized_keys`` file under ``~/.ssh/`` folder as well.\nAppend ``authorized_keys`` with the content of ``id_rsa.pub``\nThis step will make sure all the machines in the cluster is able to communicate with each other.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before moving on to next step, it is better to perform some sanity checks to make sure the communication is good.\nFor example, if you can successfully ssh into other machines, it means they can communicate with each other now. You are good to go.\nIf there is any error during ssh, you can use option ``-vvv`` to get verbose information for debugging.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once you get the cluster ready, it is time to install MXNet.\nMXNet provides a script ``tools/launch.py`` to make it easy to launch distributed training on a cluster with ssh, mpi, sge or yarn.\nLet's first install MXNet on all the machines.\n::\n\n    pip install mxnet-cu100\n\nFor more installation options (i.e., different versions of MXNet or CUDA),\nplease check `GluonCV installation guide <https://gluon-cv.mxnet.io/install/install-more.html>`_ for more information.\nOne side note, `clush <https://clustershell.readthedocs.io/en/latest/tools/clush.html>`_ is a good tool for cluster setup.\nIt can be used for executing a command concurrently on a cluster of hosts, but we won't go into details here.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we need the script to launch the job, let's clone the repo as well.\n::\n\n    git clone https://github.com/apache/incubator-mxnet.git --recursive\n\nThe script we need is under folder ``tools``, named ``launch.py``.\nNote that, this script can be put on any of the node because we only need it to launch the job.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now it is time to prepare your code and data in each node.\nThe code needs to be in the same directoty on every machine so that a single command can work on multiple machines.\nLet's clone the GluonCV repo and install it,\n::\n\n    git clone https://github.com/dmlc/gluon-cv.git\n    cd gluon-cv\n    pip install -e .\n\nSimilarly, the data needs to be in the same path on every machine as well so that the dataloader knows where to find the data.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can kickstart the training. Let's type the command below to start a training job with 4 machines.\n\n::\n\n    ../incubator-mxnet/tools/launch.py -n 4 -H host.txt --launcher ssh \\\n    python ./scripts/action-recognition/train_recognizer.py --dataset kinetics400 \\\n    --data-dir ~/.mxnet/kinetics400/train --val-data-dir ~/.mxnet/kinetics400/val \\\n    --train-list ~/.mxnet/kinetics400/train.txt --val-list ~/.mxnet/kinetics400/val.txt \\\n    --dtype float32 --mode hybrid --prefetch-ratio 1.0 --kvstore dist_sync_device \\\n    --model slowfast_4x16_resnet50_kinetics400 --slowfast --slow-temporal-stride 16 \\\n    --fast-temporal-stride 2 --video-loader --use-decord --num-classes 400 --batch-size 8 \\\n    --num-gpus 8 --num-data-workers 32 --input-size 224 --new-height 256 --new-width 340 \\\n    --new-length 64 --new-step 1 --lr-mode cosine --lr 0.4 --momentum 0.9 --wd 0.0001 \\\n    --num-epochs 196 --warmup-epochs 34 --warmup-lr 0.01 --scale-ratios 1.0,0.8 \\\n    --save-frequency 10 --log-interval 50 --logging-file slowfast_4x16.log --save-dir ./checkpoints\n\nHere, the ``host.txt`` file contains the private IP addresses of all machines, e.g.,\n::\n    123.123.1.123\n    123.123.2.123\n    123.123.3.123\n    123.123.4.123\n\n``--kvstore dist_sync_device`` is when there are multiple GPUs being used on each node, this mode aggregates gradients and updates weights on GPU.\nThere are other modes, like dist_sync, dist_async etc.\nTo find out more details, check out `this tutorial <https://mxnet.apache.org/api/faq/distributed_training>`_.\n\nAnother thing to notice is the learning rate. For single node training, we set lr to 0.1. However, for multi-node training, we increase it to 0.4\nbecause we have four machines. It's a good practice, called linear scaling rule, introduced in [Goyal17]_.\nWhen the minibatch size is multiplied by k, multiply the learning rate by k. All other hyper-parameters (weight decay, etc.) are kept unchanged.\nThe linear scaling rule can help us to not only match the accuracy between using small and large minibatches, but equally importantly, to largely\nmatch their training curves, which enables rapid debugging and comparison of experiments prior to convergence.\n\nIf everything is setup well, you will see the model is training now. All printed information will be captured and sent to the machine running launch.py.\nCheckpoints will be saved locally on each machine.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Speed\n-----\n\nUsually, the training will be faster when you use more machines, but not linear upscaling due to the communication cost.\nThe actual speed up depends on the network bandwidth, server CPU capibility, dataloader efficiency, etc.\nFor example, if you use our code on four P3.16xlarge machines on AWS in the same placement group, you will get 3x speed boost.\nSimilar speed up ratio (0.75) can be observed when you use 8 machines or more.\nIn our case, we use eight P3.16xlarge machines to train a ``slowfast_4x16_resnet50_kinetics400`` model. The training can be completed in 1.5 days.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FAQ\n---\n\nQ1: I see the error message below, what should I do?\n::\n    Permissions for 'id_rsa' are too open.\n    It is recommended that your private key files are NOT accessible by others.\n    This private key will be ignored.\n\nAnswer: you need to make sure the permission of id_rsa is good via ``chmod 400 id_rsa`` after the copy.\n\nQ2: I can't find the file ``.ssh/authorized_keys``.\n\nAnswer: Just create one with\n::\n    touch ~/.ssh/authorized_keys\n    chmod 600 ~/.ssh/authorized_keys\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n----------\n\n.. [Goyal17] Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, Kaiming He. \\\n    \"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour.\" \\\n    arXiv preprint arXiv:1706.02677 (2017).\n\n.. [Feichtenhofer18] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He. \\\n    \"SlowFast Networks for Video Recognition.\" \\\n    arXiv preprint arXiv:1812.03982 (2018).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}