{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Started with Pre-trained I3D Models on Kinetics400\n",
    "=============================================================\n",
    "\n",
    "`Kinetics400`  is an action recognition dataset\n",
    "of realistic action videos, collected from YouTube. With about 240K training and 2K validation short trimmed videos\n",
    "from 400 action categories, it is one of the most widely used dataset in the research\n",
    "community for benchmarking state-of-the-art video action recognition models.\n",
    "\n",
    "In this tutorial, we will demonstrate how to load a pre-trained model from `gluoncv-model-zoo`\n",
    "and classify videos from the Internet or your local disk into one of the 400 action classes.\n",
    "\n",
    "Step by Step\n",
    "------------\n",
    "\n",
    "We will show two exmaples here. For simplicity, we first try out a pre-trained Kinetics400 model\n",
    "on a single video clip. \n",
    "\n",
    "First, please follow the `installation guide <../../index.html#installation>`__\n",
    "to install ``MXNet`` and ``GluonCV`` if you haven't done so yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, image\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "from gluoncv.data.transforms import video\n",
    "from gluoncv import utils\n",
    "from gluoncv.model_zoo import get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we download and show the example image:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/bryanyzhu/tiny-kinetics400/raw/master/abseiling.mp4'\n",
    "im_fname = utils.download(url)\n",
    "\n",
    "img = image.imread(im_fname)\n",
    "\n",
    "plt.imshow(img.asnumpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you don't recognize it, the image is a man abseiling :)\n",
    "\n",
    "Now we define transformations for the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_fn = transforms.Compose([\n",
    "    video.VideoCenterCrop(size=224),\n",
    "    video.VideoToTensor(),\n",
    "    video.VideoNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation function does three things:\n",
    "center crop the image to 224x224 in size,\n",
    "transpose it to ``num_channels*height*width``,\n",
    "and normalize with mean and standard deviation calculated across all ImageNet images.\n",
    "\n",
    "What does the transformed image look like?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = transform_fn([img.asnumpy()])\n",
    "plt.imshow(np.transpose(img_list[0], (1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't recognize anything? *Don't panic!* Neither do I.\n",
    "The transformation makes it more \"model-friendly\", instead of \"human-friendly\".\n",
    "\n",
    "Next, we load a pre-trained model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = get_model('i3d_resnet50_v1_kinetics400', nclass=400, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you want to use InceptionV3 series model, please resize the image to have\n",
    "both dimensions larger than 299 (e.g., 340x450) and change input size from 224 to 299\n",
    "in the transform function. Finally, we prepare the image and feed it to the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = net(nd.array(img_list[0]).expand_dims(axis=0))\n",
    "\n",
    "classes = net.classes\n",
    "topK = 5\n",
    "ind = nd.topk(pred, k=topK)[0].astype('int')\n",
    "print('The input video frame is classified to be')\n",
    "for i in range(topK):\n",
    "    print('\\t[%s], with probability %.3f.'%\n",
    "          (classes[ind[i].asscalar()], nd.softmax(pred)[0][ind[i]].asscalar()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our pre-trained model predicts this video frame\n",
    "to be ``abseiling`` action with high confidence.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
