{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Getting Started with Pre-trained I3D Models on Kinetcis400\n================================================================\n\n`Kinetics400 <https://deepmind.com/research/open-source/kinetics>`_  is an action recognition dataset\nof realistic action videos, collected from YouTube. With 306,245 short trimmed videos\nfrom 400 action categories, it is one of the largest and most widely used dataset in the research\ncommunity for benchmarking state-of-the-art video action recognition models.\n\n`I3D <https://arxiv.org/abs/1705.07750>`_ (Inflated 3D Networks) is a widely adopted 3D video\nclassification network. It uses 3D convolution to learn spatiotemporal information directly from videos.\nI3D is proposed to improve `C3D <https://arxiv.org/abs/1412.0767>`_ (Convolutional 3D Networks) by inflating from 2D models.\nWe can not only reuse the 2D models' architecture (e.g., ResNet, Inception), but also bootstrap\nthe model weights from 2D pretrained models. In this manner, training 3D networks for video\nclassification is feasible and getting much better results.\n\nIn this tutorial, we will demonstrate how to load a pre-trained I3D model from `gluoncv-model-zoo`\nand classify a video clip from the Internet or your local disk into one of the 400 action classes.\n\nStep by Step\n------------\n\nWe will try out a pre-trained I3D model on a single video clip.\n\nFirst, please follow the `installation guide <../../index.html#installation>`__\nto install ``MXNet`` and ``GluonCV`` if you haven't done so yet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport mxnet as mx\nfrom mxnet import gluon, nd, image\nfrom mxnet.gluon.data.vision import transforms\nfrom gluoncv.data.transforms import video\nfrom gluoncv import utils\nfrom gluoncv.model_zoo import get_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we download the video and extract a 32-frame clip from it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gluoncv.utils.filesystem import try_import_decord\ndecord = try_import_decord()\n\nurl = 'https://github.com/bryanyzhu/tiny-ucf101/raw/master/abseiling_k400.mp4'\nvideo_fname = utils.download(url)\nvr = decord.VideoReader(video_fname)\nframe_id_list = range(0, 64, 2)\nvideo_data = vr.get_batch(frame_id_list).asnumpy()\nclip_input = [video_data[vid, :, :, :] for vid, _ in enumerate(frame_id_list)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we define transformations for the video clip.\nThis transformation function does three things:\ncenter crop the image to 224x224 in size,\ntranspose it to ``num_channels*num_frames*height*width``,\nand normalize with mean and standard deviation calculated across all ImageNet images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transform_fn = video.VideoGroupValTransform(size=224, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\nclip_input = transform_fn(clip_input)\nclip_input = np.stack(clip_input, axis=0)\nclip_input = clip_input.reshape((-1,) + (32, 3, 224, 224))\nclip_input = np.transpose(clip_input, (0, 2, 1, 3, 4))\nprint('Video data is downloaded and preprocessed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we load a pre-trained I3D model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_name = 'i3d_inceptionv1_kinetics400'\nnet = get_model(model_name, nclass=400, pretrained=True)\nprint('%s model is successfully loaded.' % model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that if you want to use InceptionV3 series model (i.e., i3d_inceptionv3_kinetics400),\nplease resize the image to have both dimensions larger than 299 (e.g., 340x450) and change input size from 224 to 299\nin the transform function. Finally, we prepare the video clip and feed it to the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pred = net(nd.array(clip_input))\n\nclasses = net.classes\ntopK = 5\nind = nd.topk(pred, k=topK)[0].astype('int')\nprint('The input video clip is classified to be')\nfor i in range(topK):\n    print('\\t[%s], with probability %.3f.'%\n          (classes[ind[i].asscalar()], nd.softmax(pred)[0][ind[i]].asscalar()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that our pre-trained model predicts this video clip\nto be ``abseiling`` action with high confidence.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next Step\n---------\n\nIf you would like to dive deeper into training I3D models on ``Kinetics400``,\nfeel free to read the next `tutorial on Kinetics400 <dive_deep_i3d_kinetics400.html>`__.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}